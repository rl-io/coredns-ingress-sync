# How It Works

This document explains the technical implementation and architecture of the coredns-ingress-sync controller.

## Architecture Overview

The coredns-ingress-sync controller follows the Kubernetes controller pattern, using controller-runtime for efficient event-driven reconciliation. It acts as a bridge between Ingress resources and CoreDNS configuration.

```
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│     Ingress     │    │                  │    │     CoreDNS     │
│   Resources     │───▶│   Controller     │───▶│  Configuration  │
│                 │    │                  │    │                 │
└─────────────────┘    └──────────────────┘    └─────────────────┘
                              │
                              ▼
                    ┌─────────────────┐
                    │ Dynamic ConfigMap│
                    │ (coredns-custom) │
                    └─────────────────┘
```

## Component Breakdown

### 1. Controller Runtime Framework

The controller uses the controller-runtime library for:

- **Event-driven reconciliation**: Responds to Ingress and ConfigMap changes
- **Leader election**: Supports multiple replicas with automatic leader selection
- **Efficient caching**: Minimizes API server load through smart caching
- **Retry logic**: Automatic retry on transient failures

```go
// Controller setup with multiple watches
mgr, err := ctrl.NewManager(ctrl.GetConfigOrDie(), ctrl.Options{
    LeaderElection:          true,
    LeaderElectionID:        "coredns-ingress-sync-leader",
    HealthProbeBindAddress:  ":8081",
})

// Watch Ingress resources
controller.Watch(
    source.Kind(mgr.GetCache(), &networkingv1.Ingress{}),
    handler.EnqueueRequestsFromMapFunc(mapIngressesToReconcile),
    predicate.Funcs{CreateFunc: isTargetIngress, UpdateFunc: hasIngressChanged},
)
```

### 2. Ingress Processing Pipeline

```
Ingress Event → Filter by IngressClass → Extract Hostnames → Generate Config → Update ConfigMap
```

#### Ingress Filtering

The controller only processes Ingresses that match the configured IngressClass:

```go
func isTargetIngress(obj client.Object) bool {
    ingress, ok := obj.(*networkingv1.Ingress)
    if !ok {
        return false
    }
    return ingress.Spec.IngressClassName != nil && 
           *ingress.Spec.IngressClassName == ingressClass
}
```

#### Hostname Extraction

Extracts all unique hostnames from matching Ingress rules:

```go
func extractHostnames(ingresses []networkingv1.Ingress) []string {
    hostSet := make(map[string]bool)
    
    for _, ing := range ingresses {
        if ing.Spec.IngressClassName == nil || *ing.Spec.IngressClassName != ingressClass {
            continue
        }
        
        for _, rule := range ing.Spec.Rules {
            if rule.Host != "" {
                hostSet[rule.Host] = true
            }
        }
    }
    
    return mapKeysToSlice(hostSet)
}
```

### 3. Configuration Generation

The controller generates CoreDNS rewrite rules for each discovered hostname:

```go
func generateDynamicConfig(hosts []string) string {
    var config strings.Builder
    
    config.WriteString("# Auto-generated by coredns-ingress-sync controller\n")
    config.WriteString(fmt.Sprintf("# Last updated: %s\n", time.Now().Format(time.RFC3339)))
    config.WriteString("\n")
    
    for _, host := range hosts {
        config.WriteString(fmt.Sprintf(
            "rewrite name exact %s %s\n", 
            host, 
            targetCNAME,
        ))
    }
    
    return config.String()
}
```

#### Example Output

```dns
# Auto-generated by coredns-ingress-sync controller
# Last updated: 2025-01-16T10:30:00Z

rewrite name exact api.example.com ingress-nginx-controller.ingress-nginx.svc.cluster.local.
rewrite name exact web.example.com ingress-nginx-controller.ingress-nginx.svc.cluster.local.
```

### 4. CoreDNS Integration

The controller integrates with CoreDNS through two mechanisms:

#### Automatic Configuration (Recommended)

When `COREDNS_AUTO_CONFIGURE=true`:

1. **Import Statement**: Adds `import /etc/coredns/custom/*.server` to CoreDNS Corefile
2. **Volume Mount**: Adds volume mount for dynamic ConfigMap
3. **Volume**: Creates ConfigMap volume reference

```go
func (r *IngressReconciler) ensureCoreDNSConfiguration(ctx context.Context) error {
    // Add import statement to Corefile
    if err := r.ensureCoreDNSImport(ctx); err != nil {
        return err
    }
    
    // Add volume mount to deployment
    if err := r.ensureCoreDNSVolumeMount(ctx); err != nil {
        return err
    }
    
    return nil
}
```

#### Manual Configuration

For environments where automatic configuration is not desired:

1. Manually add import statement to CoreDNS Corefile
2. Manually configure volume mount
3. Set `COREDNS_AUTO_CONFIGURE=false`

### 5. Dynamic ConfigMap Management

The controller manages a dedicated ConfigMap (`coredns-custom`) containing the dynamic configuration:

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns-custom
  namespace: kube-system
  labels:
    app.kubernetes.io/managed-by: coredns-ingress-sync
data:
  dynamic.server: |
    # Auto-generated by coredns-ingress-sync controller
    # Last updated: 2025-01-16T10:30:00Z
    
    rewrite name exact api.example.com ingress-nginx-controller.ingress-nginx.svc.cluster.local.
    rewrite name exact web.example.com ingress-nginx-controller.ingress-nginx.svc.cluster.local.
```

#### ConfigMap Update Strategy

The controller uses optimistic concurrency control to handle concurrent updates:

```go
func (r *IngressReconciler) updateDynamicConfigMap(ctx context.Context, hosts []string) error {
    for attempt := 0; attempt < 3; attempt++ {
        configMap := &corev1.ConfigMap{}
        
        // Fresh read on each attempt
        err := r.Get(ctx, configMapName, configMap)
        if err != nil {
            // Create if doesn't exist
            return r.createDynamicConfigMap(ctx, hosts)
        }
        
        // Check if update is actually needed
        newConfig := generateDynamicConfig(hosts)
        if configMap.Data[dynamicConfigKey] == newConfig {
            return nil // No change needed
        }
        
        // Update with new configuration
        configMap.Data[dynamicConfigKey] = newConfig
        if err := r.Update(ctx, configMap); err != nil {
            if attempt == 2 {
                return err
            }
            time.Sleep(100 * time.Millisecond) // Brief backoff
            continue
        }
        
        return nil
    }
}
```

### 6. Reconciliation Logic

The controller uses a single reconciliation function for all events:

```go
func (r *IngressReconciler) Reconcile(ctx context.Context, req reconcile.Request) (reconcile.Result, error) {
    // 1. List all ingresses in the cluster
    var ingressList networkingv1.IngressList
    if err := r.List(ctx, &ingressList); err != nil {
        return reconcile.Result{RequeueAfter: time.Minute}, err
    }
    
    // 2. Extract hostnames from matching ingresses
    hosts := extractHostnames(ingressList.Items)
    
    // 3. Update dynamic ConfigMap
    if err := r.updateDynamicConfigMap(ctx, hosts); err != nil {
        return reconcile.Result{RequeueAfter: time.Minute}, err
    }
    
    // 4. Ensure CoreDNS configuration is correct
    if err := r.ensureCoreDNSConfiguration(ctx); err != nil {
        log.Printf("Warning: Failed to ensure CoreDNS configuration: %v", err)
        // Don't fail reconciliation if CoreDNS is not available
    }
    
    return reconcile.Result{}, nil
}
```

### 7. Event Handling

The controller watches multiple resource types:

#### Ingress Events

- **Create**: New hostname added to configuration
- **Update**: Hostname changes reflected in configuration  
- **Delete**: Hostname removed from configuration

#### ConfigMap Events

- **CoreDNS ConfigMap**: Defensive configuration management
- **Dynamic ConfigMap**: External update detection

```go
// Example: Ingress create event flow
Ingress Created → Event Generated → Reconcile Triggered → 
Hostnames Extracted → Configuration Updated → ConfigMap Updated → 
CoreDNS Reloads → DNS Resolution Active
```

### 8. Error Handling and Resilience

The controller implements several resilience patterns:

#### Retry Logic

```go
// Automatic retry on reconciliation failures
if err := r.reconcileIngresses(ctx); err != nil {
    return reconcile.Result{RequeueAfter: time.Minute}, err
}
```

#### Defensive Configuration Management

Protects against external modification of CoreDNS configuration:

```go
// Watch CoreDNS ConfigMap for external changes
if !strings.Contains(corefile, importStatement) {
    log.Printf("Import statement missing, re-adding...")
    return r.ensureCoreDNSImport(ctx)
}
```

#### Graceful Degradation

The controller continues operating even if CoreDNS management fails:

```go
if err := r.ensureCoreDNSConfiguration(ctx); err != nil {
    log.Printf("Warning: CoreDNS configuration failed: %v", err)
    // Continue with ConfigMap update
}
```

## Performance Characteristics

### Memory Usage

- **Base memory**: ~32Mi for controller runtime overhead
- **Per ingress**: ~0.1Mi additional memory per ingress hostname
- **Scaling**: Linear memory growth with number of ingresses

### CPU Usage

- **Idle**: ~5m CPU when no ingress changes
- **Processing**: ~20m CPU during batch ingress updates
- **Reconciliation**: <100ms average reconciliation time

### Network Impact

- **API calls**: Minimized through controller-runtime caching
- **ConfigMap updates**: Only when actual changes detected
- **Watch efficiency**: Uses Kubernetes watch API for real-time updates

## Security Considerations

### RBAC Permissions

The controller uses minimal required permissions:

```yaml
# Cluster-wide permissions (read-only)
- apiGroups: ["networking.k8s.io"]
  resources: ["ingresses"]
  verbs: ["get", "list", "watch"]

# Namespace-scoped permissions
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]
```

### Security Context

Runs with non-root user and restricted capabilities:

```yaml
securityContext:
  allowPrivilegeEscalation: false
  capabilities:
    drop: ["ALL"]
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 65534
```

### Network Policies

Can be restricted with network policies to limit cluster network access.

## Monitoring and Observability

### Health Checks

The controller exposes health check endpoints:

- `/healthz`: Basic health check
- `/readyz`: Readiness check (considers leader election)

### Logging

Structured logging with configurable levels:

```go
log.Printf("[%s] Successfully updated ConfigMap with %d hosts", podName, len(hosts))
log.Printf("[%s] Reconciling changes for request: %s", podName, req.NamespacedName)
```

### Metrics

Future enhancement: Prometheus metrics for:

- Reconciliation count/duration
- ConfigMap update frequency
- Error rates
- Ingress processing latency
